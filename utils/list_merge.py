import requests, base64, os, json, urllib.parse

def get_content(url):
    try:
        # æ¨¡æ‹Ÿæµè§ˆå™¨å¤´éƒ¨ï¼Œé˜²æ­¢è¢«å±è”½
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        r = requests.get(url, headers=headers, timeout=30)
        return r.text if r.status_code == 200 else ""
    except:
        return ""

def start():
    # 1. è‡ªåŠ¨è¯†åˆ« GitHub ä»“åº“è·¯å¾„
    repo = os.getenv('GITHUB_REPOSITORY', 'awwy1222/V2RayAggregator')
    
    # 2. åŠ è½½æºå¹¶æ”¶å‰²èŠ‚ç‚¹
    sub_list_path = './sub/sub_list.json'
    if not os.path.exists(sub_list_path):
        print("é”™è¯¯ï¼šæ‰¾ä¸åˆ° sub/sub_list.json")
        return

    with open(sub_list_path, 'r', encoding='utf-8') as f:
        sub_list = [item for item in json.load(f) if item.get('enabled')]

    all_nodes = []
    for item in sub_list:
        print(f"æ­£åœ¨æ”¶å‰²: {item.get('remarks', 'æœªçŸ¥æº')}")
        content = get_content(item['url'])
        if content:
            try:
                # å…¼å®¹å¤„ç†ï¼šå°è¯• Base64 è§£ç 
                pure_data = content.replace('\n','').replace('\r','').strip()
                decoded = base64.b64decode(pure_data).decode('utf-8')
                nodes = decoded.split('\n')
            except:
                nodes = content.split('\n')
            
            for n in nodes:
                n = n.strip()
                if any(n.startswith(p) for p in ["vmess://", "ss://", "ssr://", "trojan://", "vless://"]):
                    all_nodes.append(n)

    # å»é‡å¤„ç†
    valid_nodes = list(set(all_nodes))
    total_count = len(valid_nodes)
    print(f"\nâœ… èŠ‚ç‚¹æ”¶å‰²å®Œæˆï¼šå…±è®¡ {total_count} ä¸ªèŠ‚ç‚¹")

    # 3. ä¿å­˜åŸå§‹èŠ‚ç‚¹åˆ° sub_merge.txt
    os.makedirs('./sub', exist_ok=True)
    with open('./sub/sub_merge.txt', 'w', encoding='utf-8') as f:
        f.write("\n".join(valid_nodes))

    # 4. å‘è½¬æ¢ API è¯·æ±‚å®Œæ•´çš„ Clash é…ç½®æ–‡ä»¶
    raw_url = f"https://raw.githubusercontent.com/{repo}/master/sub/sub_merge.txt"
    encoded_raw_url = urllib.parse.quote(raw_url)
    
    # ä½¿ç”¨ ACL4SSR è¿œç¨‹é…ç½®
    convert_api = f"https://api.v1.mk/sub?target=clash&url={encoded_raw_url}&insert=false&emoji=true&list=true&config=https%3A%2F%2Fraw.githubusercontent.com%2FACL4SSR%2FACL4SSR%2Fmaster%2FClash%2Fconfig%2FACL4SSR_Online_Full.ini"
    
    print("æ­£åœ¨å‘è½¬æ¢æœåŠ¡å™¨è¯·æ±‚ Clash å®Œæ•´é…ç½®...")
    clash_config_content = get_content(convert_api)
    
    # 5. å°†ç»“æœä¿å­˜ä¸º config.yaml
    if "proxies:" in clash_config_content:
        with open('./sub/config.yaml', 'w', encoding='utf-8') as f:
            f.write(clash_config_content)
        print("ğŸš€ å¤§åŠŸå‘Šæˆï¼å·²ç»ç”Ÿæˆå®Œæ•´é…ç½®æ–‡ä»¶ï¼šsub/config.yaml")
    else:
        # ä¿®å¤æ­¤å¤„çš„ç¼©è¿›é”™è¯¯
        with open('./sub/config.yaml', 'w', encoding='utf-8') as f:
            f.write("# è½¬æ¢ API æš‚æ—¶ç¹å¿™ï¼Œè¯·ç¨ååœ¨ Actions ä¸­é‡è·‘\nproxies: []")
        print("âš ï¸ è½¬æ¢å¤±è´¥ï¼šAPI æœªè¿”å›å†…å®¹ï¼Œè¯·æ£€æŸ¥èŠ‚ç‚¹æ•°é‡æˆ–ç¨åå†è¯•ã€‚")

if __name__ == '__main__':
    start()
