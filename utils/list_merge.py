import requests, base64, os, json, urllib.parse, time

def get_content(url, timeout_sec=60):
    """
    ä¸“é—¨å°è£…çš„å†…å®¹æŠ“å–å‡½æ•°ï¼Œæ”¯æŒè‡ªå®šä¹‰è¶…æ—¶
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        # è¿™é‡Œå°±æ˜¯ä½ è¦æ±‚çš„ï¼šè½¬æ¢è¯·æ±‚æ—¶ï¼Œè®¾ç½® 60 ç§’è¶…æ—¶
        r = requests.get(url, headers=headers, timeout=timeout_sec)
        return r.text if r.status_code == 200 else ""
    except Exception as e:
        print(f"è¯·æ±‚è¶…æ—¶æˆ–å‡ºé”™ (è®¾å®šé™åˆ¶ä¸º {timeout_sec}s): {e}")
        return ""

def start():
    repo = os.getenv('GITHUB_REPOSITORY', 'awwy1222/V2RayAggregator')
    raw_url = f"https://raw.githubusercontent.com/{repo}/master/sub/sub_merge.txt"
    
    # 1. åŠ è½½æºå¹¶æ”¶å‰²
    sub_list_path = './sub/sub_list.json'
    with open(sub_list_path, 'r', encoding='utf-8') as f:
        sub_list = [item for item in json.load(f) if item.get('enabled')]

    all_nodes = []
    for item in sub_list:
        print(f"æ­£åœ¨æ”¶å‰²: {item.get('remarks', 'æœªçŸ¥æº')}")
        # æŠ“å–æºçš„åˆ—è¡¨é€šå¸¸å¾ˆå¿«ï¼Œé»˜è®¤ 30 ç§’å¤Ÿäº†
        content = get_content(item['url'], timeout_sec=30)
        if content:
            try:
                pure_data = content.replace('\n','').replace('\r','').strip()
                decoded = base64.b64decode(pure_data).decode('utf-8')
                nodes = decoded.split('\n')
            except:
                nodes = content.split('\n')
            for n in nodes:
                n = n.strip()
                if any(n.startswith(p) for p in ["vmess://", "ss://", "ssr://", "trojan://", "vless://"]):
                    all_nodes.append(n)

    valid_nodes = list(set(all_nodes))
    print(f"\nâœ… èŠ‚ç‚¹æ”¶å‰²å®Œæˆï¼šå…±è®¡ {len(valid_nodes)} ä¸ªèŠ‚ç‚¹")

    # 2. ä¿å­˜åŸå§‹æ–‡æœ¬
    os.makedirs('./sub', exist_ok=True)
    with open('./sub/sub_merge.txt', 'w', encoding='utf-8') as f:
        f.write("\n".join(valid_nodes))

    # 3. æ ¸å¿ƒï¼šåœ¨çº¿è½¬æ¢é…ç½® (è®¾ç½® 60 ç§’è¶…æ—¶)
    encoded_raw_url = urllib.parse.quote(raw_url)
    online_api = f"https://api.v1.mk/sub?target=clash&url={encoded_raw_url}&insert=false&emoji=true&list=true&config=https%3A%2F%2Fraw.githubusercontent.com%2FACL4SSR%2FACL4SSR%2Fmaster%2FClash%2Fconfig%2FACL4SSR_Online_Full.ini"
    
    print(f"ğŸ”„ æ­£åœ¨å°è¯•åœ¨çº¿è½¬æ¢ï¼Œå·²è®¾ç½® 60 ç§’è¶…æ—¶ç­‰å¾…...")
    
    # ã€å…³é”®ç‚¹ã€‘è¿™é‡Œè°ƒç”¨å‡½æ•°ï¼Œä¼ å…¥ 60 ç§’
    clash_config = get_content(online_api, timeout_sec=60)

    if "proxies:" in clash_config:
        with open('./sub/config.yaml', 'w', encoding='utf-8') as f:
            f.write(clash_config)
        print("ğŸš€ [ç²¾ä¿®ç‰ˆ] åœ¨çº¿è½¬æ¢æˆåŠŸï¼")
    else:
        print("âš ï¸ 1 åˆ†é’Ÿå†…æœªæ”¶åˆ°åœ¨çº¿ API å“åº”ï¼Œå¯ç”¨æœ¬åœ°ä¿åº•æ–¹æ¡ˆ...")
        local_template = f"""
mixed-port: 7890
allow-lan: true
mode: rule
log-level: info
proxy-providers:
  my_nodes:
    type: http
    url: "{raw_url}"
    interval: 3600
    path: ./sub_merge.txt
    health-check:
      enable: true
      interval: 600
      url: http://www.gstatic.com/generate_204
proxy-groups:
  - name: ğŸš€ è‡ªåŠ¨é€‰æ‹©
    type: url-test
    use: [my_nodes]
  - name: ğŸ¯ æ‰‹åŠ¨åˆ‡æ¢
    type: select
    use: [my_nodes]
rules:
  - GEOIP,CN,DIRECT
  - MATCH,ğŸš€ è‡ªåŠ¨é€‰æ‹©
"""
        with open('./sub/config.yaml', 'w', encoding='utf-8') as f:
            f.write(local_template)
        print("ğŸ“¦ [æœ¬åœ°ç‰ˆ] ä¿åº•é…ç½®å·²ç”Ÿæˆï¼")

if __name__ == '__main__':
    start()
