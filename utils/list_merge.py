import requests, base64, os, json, urllib.parse, re, socket

def get_content(url, timeout_sec=60):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        r = requests.get(url, headers=headers, timeout=timeout_sec)
        return r.text if r.status_code == 200 else ""
    except: return ""

def check_port(host, port):
    """è½»é‡çº§ TCP ç«¯å£æ£€æµ‹ï¼šç­›é€‰æ‰ç»å¯¹ä¸èƒ½ç”¨çš„æ­»èŠ‚ç‚¹"""
    try:
        with socket.create_connection((host, int(port)), timeout=2):
            return True
    except: return False

def extract_info(node_str):
    """è§£æèŠ‚ç‚¹ IP å’Œç«¯å£ï¼Œç”¨äºå»é‡å’Œå­˜æ´»æ£€æµ‹"""
    try:
        if node_str.startswith("ss://"):
            content = node_str.split("//")[1].split("#")[0]
            if "@" in content: host_port = content.split("@")[1]
            else:
                padding = '=' * (4 - len(content) % 4)
                decoded = base64.b64decode(content + padding).decode('utf-8', errors='ignore')
                host_port = decoded.split("@")[1] if "@" in decoded else decoded
        elif "://" in node_str:
            parts = node_str.split("@")
            if len(parts) > 1: host_port = parts[1].split("?")[0]
            else: return None, None
        
        if ":" in host_port:
            h, p = host_port.split(":")[0], host_port.split(":")[1].split("/")[0]
            return h, p
    except: pass
    return None, None

def start():
    repo = os.getenv('GITHUB_REPOSITORY', 'awwy1222/V2RayAggregator')
    raw_url = f"https://raw.githubusercontent.com/{repo}/master/sub/sub_merge.txt"
    
    # 1. åŠ è½½æºå¹¶æ”¶å‰²
    with open('./sub/sub_list.json', 'r', encoding='utf-8') as f:
        sub_list = [item for item in json.load(f) if item.get('enabled')]

    all_nodes = []
    seen_features = set() 

    for item in sub_list:
        print(f"æ”¶å‰²å¹¶ç­›é€‰: {item.get('remarks')}")
        content = get_content(item['url'], 30)
        if not content: continue
        
        try:
            padding = '=' * (4 - len(content.strip()) % 4)
            nodes = base64.b64decode(content.strip() + padding).decode('utf-8', errors='ignore').split('\n')
        except: nodes = content.split('\n')
        
        for n in nodes:
            n = n.strip()
            if not any(n.startswith(p) for p in ["vmess://", "ss://", "ssr://", "trojan://", "vless://"]): continue
            
            host, port = extract_info(n)
            if host and port:
                feature = f"{host}:{port}"
                # [è¦æ±‚1] æ ¹æ® IP å’Œç«¯å£å»é‡
                if feature not in seen_features:
                    # [è¦æ±‚2] ç­›é€‰æ‰ä¸èƒ½ç”¨çš„ï¼ˆç«¯å£ä¸é€šçš„ç›´æ¥è¸¢å‡ºï¼‰
                    if check_port(host, port):
                        all_nodes.append(n)
                        seen_features.add(feature)

    print(f"âœ… ç­›é€‰å»é‡å®Œæˆï¼šå‰©ä½™ {len(all_nodes)} ä¸ªå¯ç”¨èŠ‚ç‚¹")
    os.makedirs('./sub', exist_ok=True)
    with open('./sub/sub_merge.txt', 'w', encoding='utf-8') as f:
        f.write("\n".join(all_nodes))

    # [è¦æ±‚4] è½¬æ¢é—®é¢˜ï¼šå…ˆ API è½¬æ¢ï¼Œè®¾ç½® 2 åˆ†é’Ÿè¶…æ—¶
    encoded_raw_url = urllib.parse.quote(raw_url)
    online_api = f"https://api.v1.mk/sub?target=clash&url={encoded_raw_url}&insert=false&emoji=true&list=true&config=https%3A%2F%2Fraw.githubusercontent.com%2FACL4SSR%2FACL4SSR%2Fmaster%2FClash%2Fconfig%2FACL4SSR_Online_Full.ini"
    
    print(f"ğŸ”„ æ­£åœ¨å‘èµ·åœ¨çº¿è½¬æ¢ï¼Œè®¾ç½® 120 ç§’è¶…æ—¶ç­‰å¾…...")
    clash_config = get_content(online_api, timeout_sec=120)

    # åˆ¤æ–­ API è¿”å›æ˜¯å¦æœ‰æ•ˆ
    if "proxies:" in clash_config:
        with open('./sub/config.yaml', 'w', encoding='utf-8') as f:
            f.write(clash_config)
        print("ğŸš€ [ç²¾ä¿®ç‰ˆ] åœ¨çº¿è½¬æ¢æˆåŠŸï¼")
    else:
        # [è¦æ±‚4] 2 åˆ†é’Ÿä¸ç»™ç»“æœï¼Œåˆ‡æ¢æˆæœ¬åœ°
        print("âš ï¸ API è¶…æ—¶æˆ–å¤±è´¥ï¼Œåˆ‡æ¢æˆæœ¬åœ°ä¿åº•é…ç½®...")
        # [è¦æ±‚3] Gemini ä¸“å±ç»„é€»è¾‘
        local_template = f"""
mixed-port: 7890
allow-lan: true
mode: rule
log-level: info
ipv6: false

proxy-providers:
  my_nodes:
    type: http
    url: "{raw_url}"
    interval: 3600
    path: ./nodes_data.txt
    health-check:
      enable: true
      interval: 600
      url: http://www.gstatic.com/generate_204

proxy-groups:
  - name: ğŸ¤– Gemini ä¸“ç”¨
    type: url-test
    use: [my_nodes]
    url: 'https://generativelanguage.googleapis.com/v1beta/models'
    interval: 300
    tolerance: 50
  
  - name: ğŸš€ è‡ªåŠ¨é€‰æ‹©
    type: url-test
    use: [my_nodes]
    url: 'http://www.gstatic.com/generate_204'
    interval: 300

  - name: ğŸ¯ æ‰‹åŠ¨åˆ‡æ¢
    type: select
    use: [my_nodes]

rules:
  - DOMAIN-SUFFIX,gemini.google.com,ğŸ¤– Gemini ä¸“ç”¨
  - DOMAIN-SUFFIX,generativelanguage.googleapis.com,ğŸ¤– Gemini ä¸“ç”¨
  - DOMAIN-SUFFIX,ai.google.dev,ğŸ¤– Gemini ä¸“ç”¨
  - GEOIP,CN,DIRECT
  - MATCH,ğŸš€ è‡ªåŠ¨é€‰æ‹©
"""
        with open('./sub/config.yaml', 'w', encoding='utf-8') as f:
            f.write(local_template)
        print("ğŸ“¦ [æœ¬åœ°ç‰ˆ] ä¿åº•é…ç½®å·²ç”Ÿæˆï¼å·²åŒ…å« Gemini ä¸“ç”¨æ¢æµ‹ç»„ã€‚")

if __name__ == '__main__':
    start()
