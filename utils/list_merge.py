import requests, base64, os, json, urllib.parse

def get_content(url):
    try:
        # æ¨¡æ‹Ÿæµè§ˆå™¨å¤´éƒ¨ï¼Œé˜²æ­¢è¢«å±è”½
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        r = requests.get(url, headers=headers, timeout=30)
        return r.text if r.status_code == 200 else ""
    except: return ""

def start():
    # 1. è‡ªåŠ¨è¯†åˆ« GitHub ä»“åº“è·¯å¾„ (ç”¨äºæ„é€  Raw é“¾æ¥ä¾› API è¯»å–)
    repo = os.getenv('GITHUB_REPOSITORY', 'awwy1222/V2RayAggregator')
    
    # 2. ä» sub_list.json åŠ è½½æºå¹¶æ”¶å‰²èŠ‚ç‚¹
    sub_list_path = './sub/sub_list.json'
    with open(sub_list_path, 'r', encoding='utf-8') as f:
        sub_list = [item for item in json.load(f) if item.get('enabled')]

    all_nodes = []
    for item in sub_list:
        print(f"æ­£åœ¨æ”¶å‰²: {item.get('remarks', 'æœªçŸ¥æº')}")
        content = get_content(item['url'])
        if content:
            try:
                # å…¼å®¹å¤„ç†ï¼šå°è¯• Base64 è§£ç ï¼Œå¤±è´¥åˆ™æŒ‰æ˜æ–‡å¤„ç†
                pure_data = content.replace('\n','').replace('\r','').strip()
                decoded = base64.b64decode(pure_data).decode('utf-8')
                nodes = decoded.split('\n')
            except:
                nodes = content.split('\n')
            
            for n in nodes:
                n = n.strip()
                if any(n.startswith(p) for p in ["vmess://", "ss://", "ssr://", "trojan://", "vless://"]):
                    all_nodes.append(n)

    # å»é‡å¤„ç†
    valid_nodes = list(set(all_nodes))
    total_count = len(valid_nodes)
    print(f"\nâœ… èŠ‚ç‚¹æ”¶å‰²å®Œæˆï¼šå…±è®¡ {total_count} ä¸ªèŠ‚ç‚¹")

    # 3. ä¿å­˜åŸå§‹èŠ‚ç‚¹åˆ° sub_merge.txt (è¿™æ˜¯ç»™ API è¯»çš„â€œåŸææ–™â€)
    os.makedirs('./sub', exist_ok=True)
    with open('./sub/sub_merge.txt', 'w', encoding='utf-8') as f:
        f.write("\n".join(valid_nodes))

    # 4. ã€æ ¸å¿ƒæ­¥éª¤ã€‘å‘è½¬æ¢ API è¯·æ±‚å®Œæ•´çš„ Clash é…ç½®æ–‡ä»¶
    # æ„é€ ä½ ä»“åº“ä¸­ sub_merge.txt çš„åŸå§‹åœ°å€
    raw_url = f"https://raw.githubusercontent.com/{repo}/master/sub/sub_merge.txt"
    encoded_raw_url = urllib.parse.quote(raw_url)
    
    # ä½¿ç”¨ ACL4SSR è¿œç¨‹é…ç½®ï¼Œä¼šè‡ªåŠ¨å¸®ä½ åˆ†å¥½ï¼šé¦™æ¸¯ã€ç¾å›½ã€æ—¥æœ¬ã€è‡ªåŠ¨é€‰æ‹©ç­‰ç­–ç•¥ç»„
    # è¿™æ˜¯ç›®å‰æœ€ç¾è§‚ã€æœ€ç¨³çš„è½¬æ¢ API
    convert_api = f"https://api.v1.mk/sub?target=clash&url={encoded_raw_url}&insert=false&emoji=true&list=true&config=https%3A%2F%2Fraw.githubusercontent.com%2FACL4SSR%2FACL4SSR%2Fmaster%2FClash%2Fconfig%2FACL4SSR_Online_Full.ini"
    
    print("æ­£åœ¨æŠ“å– Clash å®Œæ•´é…ç½®ä»£ç ...")
    clash_config_content = get_content(convert_api)
    
    # 5. å°†æŠ“å–å›æ¥çš„é…ç½®ä»£ç ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶
    if "proxies:" in clash_config_content:
        # ä¿å­˜ä¸º config.yaml (è¿™ä¸ªå°±æ˜¯ä½ ç›´æ¥å¯ä»¥ Raw çš„æ–‡ä»¶)
        with open('./sub/config.yaml', 'w', encoding='utf-8') as f:
            f.write(clash_config_content)
        print("ğŸš€ å¤§åŠŸå‘Šæˆï¼å·²ç»ç”Ÿæˆå®Œæ•´é…ç½®æ–‡ä»¶ï¼šsub/config.yaml")
    else:
        # å¦‚æœ API
